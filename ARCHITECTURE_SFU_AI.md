# Kiáº¿n trÃºc Real-time AI Analysis vá»›i SFU + aiortc (1 Proctor)

## ğŸ¯ Má»¥c tiÃªu
- Backend nháº­n WebRTC streams tá»« nhiá»u candidates
- Xá»­ lÃ½ AI real-time (face detection, OCR, audio analysis)
- Forward streams Ä‘áº¿n **1 proctor duy nháº¥t**
- Generate incidents tá»± Ä‘á»™ng

## ğŸ“Š Simple Overview

```
[Candidate 1]  â”€â”€â”
[Candidate 2]  â”€â”€â”¤ WebRTC Streams
[Candidate 3]  â”€â”€â”¤ (camera + screen + audio)
[Candidate N]  â”€â”€â”¤
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Backend    â”‚
         â”‚    (SFU +     â”‚
         â”‚   AI Analysis)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                     â”‚
      â–¼                     â–¼
[AI Incidents]      [Forwarded Streams]
      â”‚                     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
           [1 Proctor]
         (Xem táº¥t cáº£ + Alerts)
```

---

## Tá»•ng quan kiáº¿n trÃºc chi tiáº¿t

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CANDIDATE BROWSER(S)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Camera     â”‚         â”‚    Screen    â”‚         â”‚     Mic      â”‚         â”‚
â”‚  â”‚  MediaStream â”‚         â”‚ MediaStream  â”‚         â”‚ MediaStream  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                        â”‚                        â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                  â”‚                                          â”‚
â”‚                         RTCPeerConnection                                   â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â”‚ WebRTC (video/audio tracks)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          BACKEND SERVER (Python)                             â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    SFU Manager (aiortc)                                 â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  RTCPeerConnection (per Candidate)                               â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  - Receive video tracks (camera + screen)                        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  - Receive audio track                                           â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  - Forward to single Proctor                                     â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                â”‚                                         â”‚                   â”‚
â”‚                â”‚ Forward tracks                          â”‚                   â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚        â”‚  Frame Buffer  â”‚                       â”‚  Audio Buffer  â”‚          â”‚
â”‚        â”‚  Queue         â”‚                       â”‚  Queue         â”‚          â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                â”‚                                         â”‚                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚    â”‚                           â”‚                                             â”‚
â”‚    â–¼                           â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚   AI Processing      â”‚  â”‚  Audio Processing    â”‚                         â”‚
â”‚  â”‚   Pipeline           â”‚  â”‚  Pipeline            â”‚                         â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚                         â”‚
â”‚  â”‚  1. Face Detection   â”‚  â”‚  1. Voice Activity   â”‚                         â”‚
â”‚  â”‚     - YOLO/MTCNN     â”‚  â”‚     - VAD            â”‚                         â”‚
â”‚  â”‚  2. Face Recognition â”‚  â”‚  2. Speech Detection â”‚                         â”‚
â”‚  â”‚     - ArcFace        â”‚  â”‚  3. Background Noise â”‚                         â”‚
â”‚  â”‚  3. Pose Estimation  â”‚  â”‚                      â”‚                         â”‚
â”‚  â”‚  4. Object Detection â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚  â”‚  5. OCR (screen)     â”‚             â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚                                     â”‚
â”‚             â”‚                          â”‚                                     â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                            â”‚                                                 â”‚
â”‚                            â–¼                                                 â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                  â”‚   Rules Engine       â”‚                                    â”‚
â”‚                  â”‚  - Detect violations â”‚                                    â”‚
â”‚                  â”‚  - Generate incidentsâ”‚                                    â”‚
â”‚                  â”‚  - Calculate scores  â”‚                                    â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                             â”‚                                                â”‚
â”‚                             â–¼                                                â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                  â”‚   WebSocket          â”‚                                    â”‚
â”‚                  â”‚   To Proctor         â”‚                                    â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Real-time incidents & streams
                              â”‚
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚     PROCTOR         â”‚
                   â”‚     Browser         â”‚
                   â”‚  - View all streams â”‚
                   â”‚  - See AI alerts    â”‚
                   â”‚  - Control exam     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Chi tiáº¿t cÃ¡c thÃ nh pháº§n

### 1. **Frontend (Candidate)**

#### 1.1 WebRTC Setup
```javascript
// Gá»­i camera + screen + audio Ä‘áº¿n backend
const pc = new RTCPeerConnection()

// Camera stream
const cameraStream = await getUserMedia({video: true, audio: true})
cameraStream.getTracks().forEach(track => {
  pc.addTrack(track, cameraStream)
})

// Screen stream
const screenStream = await getDisplayMedia({video: true})
screenStream.getTracks().forEach(track => {
  pc.addTrack(track, screenStream)
})

// Create offer vÃ  gá»­i Ä‘áº¿n backend
const offer = await pc.createOffer()
await pc.setLocalDescription(offer)

// Send qua WebSocket
websocket.send(JSON.stringify({
  type: 'offer',
  sdp: pc.localDescription,
  trackInfo: [
    {trackId: 'xxx', label: 'camera', kind: 'video'},
    {trackId: 'yyy', label: 'screen', kind: 'video'},
    {trackId: 'zzz', label: 'audio', kind: 'audio'}
  ]
}))
```

---

### 2. **Backend SFU (aiortc)**

#### 2.1 Nháº­n WebRTC Tracks
```python
from aiortc import RTCPeerConnection, VideoStreamTrack, AudioStreamTrack

class CandidateConnection:
    def __init__(self, pc: RTCPeerConnection):
        self.pc = pc
        self.camera_track = None
        self.screen_track = None
        self.audio_track = None
        
        @pc.on("track")
        async def on_track(track):
            if isinstance(track, VideoStreamTrack):
                # Identify camera vs screen by trackInfo
                if track_label == 'camera':
                    self.camera_track = track
                    asyncio.create_task(self.process_camera_frames(track))
                elif track_label == 'screen':
                    self.screen_track = track
                    asyncio.create_task(self.process_screen_frames(track))
            
            elif isinstance(track, AudioStreamTrack):
                self.audio_track = track
                asyncio.create_task(self.process_audio_frames(track))
    
    async def process_camera_frames(self, track):
        """Process camera frames for AI analysis"""
        while True:
            frame = await track.recv()  # av.VideoFrame
            # Frame cÃ³ Ä‘á»‹nh dáº¡ng: width, height, format (yuv420p, rgb24, etc.)
            
            # Convert to numpy array
            img = frame.to_ndarray(format="bgr24")
            
            # Send to AI pipeline
            await ai_processor.process_camera_frame(img, self.user_id)
    
    async def process_screen_frames(self, track):
        """Process screen frames for OCR"""
        while True:
            frame = await track.recv()
            img = frame.to_ndarray(format="bgr24")
            
            # Send to OCR pipeline (lower frequency)
            await ai_processor.process_screen_frame(img, self.user_id)
    
    async def process_audio_frames(self, track):
        """Process audio for voice detection"""
        while True:
            frame = await track.recv()  # av.AudioFrame
            # Audio processing
            await ai_processor.process_audio(frame, self.user_id)
```

#### 2.2 Frame Buffer Queue
```python
import asyncio
from collections import deque

class FrameBuffer:
    """Buffer frames Ä‘á»ƒ xá»­ lÃ½ báº¥t Ä‘á»“ng bá»™"""
    def __init__(self, maxsize=30):
        self.queue = asyncio.Queue(maxsize=maxsize)
        self.last_frame = None
    
    async def put(self, frame):
        """Add frame, drop oldest if full"""
        try:
            self.queue.put_nowait(frame)
            self.last_frame = frame
        except asyncio.QueueFull:
            # Drop oldest frame
            try:
                self.queue.get_nowait()
                self.queue.put_nowait(frame)
            except:
                pass
    
    async def get(self):
        """Get next frame for processing"""
        return await self.queue.get()
    
    def get_latest(self):
        """Get latest frame (non-blocking)"""
        return self.last_frame
```

---

### 3. **AI Processing Pipeline**

#### 3.1 Face Detection & Recognition
```python
import cv2
import numpy as np
from ultralytics import YOLO
import insightface

class FaceAnalyzer:
    def __init__(self):
        self.face_detector = YOLO('yolov8n-face.pt')
        self.face_recognizer = insightface.app.FaceAnalysis()
        self.face_recognizer.prepare(ctx_id=0)
        self.registered_faces = {}  # user_id -> embedding
    
    async def analyze_frame(self, frame, user_id):
        """
        Analyze frame for face violations
        Returns: {
            'face_count': int,
            'is_registered_face': bool,
            'pose_angle': float,
            'confidence': float
        }
        """
        # Detect faces
        results = self.face_detector(frame)
        faces = results[0].boxes
        
        face_count = len(faces)
        
        if face_count == 0:
            return {
                'violation': 'A1',  # Máº¥t khuÃ´n máº·t
                'face_count': 0,
                'confidence': 0.0
            }
        
        if face_count > 1:
            return {
                'violation': 'A2',  # Nhiá»u khuÃ´n máº·t
                'face_count': face_count,
                'confidence': 0.9
            }
        
        # Get face embedding for verification
        face_embedding = self.face_recognizer.get(frame)[0].embedding
        
        # Compare with registered face
        if user_id in self.registered_faces:
            registered = self.registered_faces[user_id]
            similarity = np.dot(face_embedding, registered) / (
                np.linalg.norm(face_embedding) * np.linalg.norm(registered)
            )
            
            if similarity < 0.6:
                return {
                    'violation': 'A10',  # Nghi ngá» giáº£ máº¡o
                    'face_count': 1,
                    'confidence': 1 - similarity
                }
        
        return {
            'violation': None,
            'face_count': 1,
            'confidence': 1.0
        }
```

#### 3.2 Screen OCR Analysis
```python
import pytesseract
from PIL import Image

class ScreenAnalyzer:
    def __init__(self):
        self.blacklist_keywords = [
            'chatgpt', 'google', 'stackoverflow', 
            'answer', 'cheat', 'solution'
        ]
    
    async def analyze_screen(self, frame, user_id):
        """
        OCR screen for blacklisted content
        """
        # Convert to PIL Image
        img = Image.fromarray(frame)
        
        # OCR
        text = pytesseract.image_to_string(img).lower()
        
        # Check blacklist
        detected = [kw for kw in self.blacklist_keywords if kw in text]
        
        if detected:
            return {
                'violation': 'A5',  # TÃ i liá»‡u cáº¥m
                'detected_keywords': detected,
                'confidence': 0.8
            }
        
        return {'violation': None}
```

#### 3.3 Audio Analysis
```python
import webrtcvad

class AudioAnalyzer:
    def __init__(self):
        self.vad = webrtcvad.Vad(3)  # Aggressiveness 3
        self.speech_duration = {}  # user_id -> duration
    
    async def analyze_audio(self, audio_frame, user_id):
        """
        Detect voice activity
        """
        # Convert audio frame to bytes
        audio_data = audio_frame.to_ndarray()
        
        # Check if speech
        is_speech = self.vad.is_speech(audio_data.tobytes(), sample_rate=16000)
        
        if is_speech:
            self.speech_duration[user_id] = self.speech_duration.get(user_id, 0) + 0.02
            
            # Alert if speaking too long
            if self.speech_duration[user_id] > 30:  # 30 seconds
                return {
                    'violation': 'A6',  # Ã‚m thanh há»™i thoáº¡i
                    'duration': self.speech_duration[user_id]
                }
        else:
            # Reset counter if silence
            self.speech_duration[user_id] = 0
        
        return {'violation': None}
```

---

### 4. **AI Processor Coordinator**

```python
class AIProcessor:
    def __init__(self):
        self.face_analyzer = FaceAnalyzer()
        self.screen_analyzer = ScreenAnalyzer()
        self.audio_analyzer = AudioAnalyzer()
        
        # Processing rates
        self.camera_fps = 5  # Process 5 fps for face detection
        self.screen_interval = 6  # Process screen every 6 seconds
        self.audio_rate = 50  # Process audio every 50ms
        
        # Counters
        self.frame_counters = {}
        self.last_screen_time = {}
    
    async def process_camera_frame(self, frame, user_id):
        """Process camera frame at controlled rate"""
        # Rate limiting
        count = self.frame_counters.get(user_id, 0)
        self.frame_counters[user_id] = count + 1
        
        # Process every N frames to achieve target FPS
        if count % (30 // self.camera_fps) != 0:
            return
        
        # Analyze
        result = await self.face_analyzer.analyze_frame(frame, user_id)
        
        # If violation detected, send incident
        if result.get('violation'):
            await self.send_incident(user_id, result)
    
    async def process_screen_frame(self, frame, user_id):
        """Process screen frame at controlled rate"""
        now = time.time()
        last_time = self.last_screen_time.get(user_id, 0)
        
        if now - last_time < self.screen_interval:
            return
        
        self.last_screen_time[user_id] = now
        
        # Analyze
        result = await self.screen_analyzer.analyze_screen(frame, user_id)
        
        if result.get('violation'):
            await self.send_incident(user_id, result)
    
    async def process_audio(self, audio_frame, user_id):
        """Process audio frame"""
        result = await self.audio_analyzer.analyze_audio(audio_frame, user_id)
        
        if result.get('violation'):
            await self.send_incident(user_id, result)
    
    async def send_incident(self, user_id, result):
        """Send incident to WebSocket clients"""
        incident = {
            'type': 'incident',
            'userId': user_id,
            'tag': result['violation'],
            'confidence': result.get('confidence', 0.5),
            'timestamp': time.time(),
            'data': result
        }
        
        # Broadcast via WebSocket
        await websocket_manager.broadcast_incident(incident)
```

---

### 5. **Forward Streams Ä‘áº¿n Proctor (ÄÆ¡n giáº£n hÃ³a)**

```python
class SFUManager:
    def __init__(self):
        # Candidate connections: user_id -> CandidateConnection
        self.candidates = {}
        
        # Single proctor connection
        self.proctor_pc = None
        self.proctor_user_id = None
    
    async def handle_proctor_connection(self, proctor_user_id):
        """Setup single proctor connection"""
        self.proctor_user_id = proctor_user_id
        self.proctor_pc = RTCPeerConnection()
        
        # Add all existing candidate tracks to proctor
        for candidate_id, candidate_conn in self.candidates.items():
            if candidate_conn.camera_track:
                self.proctor_pc.addTrack(candidate_conn.camera_track)
            if candidate_conn.screen_track:
                self.proctor_pc.addTrack(candidate_conn.screen_track)
            if candidate_conn.audio_track:
                self.proctor_pc.addTrack(candidate_conn.audio_track)
        
        # Create offer to proctor
        offer = await self.proctor_pc.createOffer()
        await self.proctor_pc.setLocalDescription(offer)
        
        return offer
    
    async def on_new_candidate_track(self, track, candidate_id):
        """When new candidate connects, forward to proctor"""
        if self.proctor_pc:
            self.proctor_pc.addTrack(track)
            
            # Renegotiate with proctor
            offer = await self.proctor_pc.createOffer()
            await self.proctor_pc.setLocalDescription(offer)
            
            # Send offer to proctor via WebSocket
            await self.send_to_proctor({
                'type': 'renegotiate',
                'sdp': offer
            })
```

---

## Data Flow Timeline

```
Time: 0ms
â”œâ”€ Candidate: Camera frame captured
â”œâ”€ WebRTC: Frame sent to backend
â””â”€ Backend: Frame received (av.VideoFrame)

Time: 10ms
â”œâ”€ Backend: Frame added to buffer
â””â”€ AI Pipeline: Frame dequeued

Time: 50ms
â”œâ”€ AI: Face detection running (YOLO inference ~40ms)
â””â”€ AI: Face recognition running (ArcFace inference ~10ms)

Time: 60ms
â”œâ”€ AI: Results ready
â”œâ”€ Rules Engine: Check violations
â””â”€ If violation â†’ Generate incident

Time: 65ms
â”œâ”€ WebSocket: Broadcast incident to proctors
â””â”€ Proctor: Receive alert in real-time

Time: 100ms (next frame cycle)
â””â”€ Repeat...
```

---

## Performance Considerations

### 1. **Processing Rate**
- Camera: 5 FPS (process every 6th frame from 30fps stream)
- Screen: Every 6 seconds (OCR is expensive)
- Audio: Every 50ms (20 FPS)

### 2. **GPU Usage**
- YOLO face detection: ~40ms per frame on GPU
- ArcFace: ~10ms per face
- Target: Process 5 fps = 200ms per frame budget âœ“

### 3. **Memory (Simplified for 1 Proctor)**
- Frame buffer: Max 30 frames Ã— 1920Ã—1080Ã—3 bytes = ~180MB per candidate
- With 10 candidates: ~1.8GB RAM
- **No duplication**: Single proctor receives forwarded tracks (khÃ´ng cáº§n duplicate)

### 4. **Network Bandwidth (Simplified for 1 Proctor)**
- **Incoming**: 2 Mbps per candidate (1080p @ 30fps)
  - 10 candidates = 20 Mbps in
- **Outgoing to proctor**: 2 Mbps Ã— sá»‘ candidate Ä‘ang xem
  - Náº¿u proctor xem 10 candidates = 20 Mbps out
- **Total**: 20 Mbps in + 20 Mbps out = **40 Mbps** (ráº¥t kháº£ thi)

---

## Technology Stack

### Backend
- **FastAPI**: REST API + WebSocket
- **aiortc**: WebRTC implementation in Python
- **OpenCV**: Image processing
- **YOLO (Ultralytics)**: Face detection
- **InsightFace (ArcFace)**: Face recognition
- **Tesseract/PaddleOCR**: Screen OCR
- **webrtcvad**: Voice activity detection

### Dependencies
```bash
pip install fastapi uvicorn[standard]
pip install aiortc opencv-python numpy
pip install ultralytics insightface
pip install pytesseract pillow
pip install webrtcvad
```

---

## Deployment Architecture (Simplified for 1 Proctor)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend (React + Vite)         â”‚
â”‚    - Candidate pages (nhiá»u)            â”‚
â”‚    - Proctor page (1)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Backend        â”‚
        â”‚   FastAPI        â”‚
        â”‚   + aiortc       â”‚
        â”‚   + GPU (YOLO)   â”‚
        â”‚                  â”‚
        â”‚ Max: 10-20 candidatesâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                       â”‚
      â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Redis      â”‚      â”‚  PostgreSQL  â”‚
â”‚  (optional)  â”‚      â”‚  (Incidents) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Note**: Vá»›i 1 proctor, khÃ´ng cáº§n load balancer hay clustering phá»©c táº¡p

---

## ğŸ¯ Simplified Flow (1 Proctor)

### Connection Flow:
```
1. Proctor connects â†’ Backend táº¡o RTCPeerConnection cho proctor
2. Candidate 1 connects â†’ Backend nháº­n tracks â†’ Forward Ä‘áº¿n proctor
3. Candidate 2 connects â†’ Backend nháº­n tracks â†’ Forward Ä‘áº¿n proctor
4. Candidate N connects â†’ Backend nháº­n tracks â†’ Forward Ä‘áº¿n proctor
...
```

### Data Flow per Candidate:
```
Candidate
  â””â”€> Camera (2 Mbps) â”€â”€â”
  â””â”€> Screen (2 Mbps) â”€â”€â”¤
  â””â”€> Audio (128 Kbps) â”€â”¤
                        â”‚
                        â–¼
                   Backend (SFU)
                        â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                â”‚
                â–¼                â–¼
           AI Analysis      Forward to
           (async)          Proctor
                â”‚                â”‚
                â–¼                â”‚
           Incidents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                           Proctor sees:
                           - All candidate streams
                           - AI-generated alerts
```

### Advantages of 1 Proctor:
âœ… **ÄÆ¡n giáº£n hÆ¡n**: KhÃ´ng cáº§n quáº£n lÃ½ multiple proctor connections
âœ… **Ãt bandwidth hÆ¡n**: Chá»‰ forward 1 láº§n, khÃ´ng duplicate
âœ… **Easier debugging**: 1 connection path duy nháº¥t
âœ… **Lower latency**: Ãt hop, Ã­t processing

### Scale Limits:
- **Max candidates**: ~10-20 (giá»›i háº¡n bá»Ÿi proctor browser rendering)
- **Bandwidth**: 40 Mbps (20 in + 20 out) - OK cho 10 candidates
- **CPU/GPU**: YOLO processing 5fps Ã— 10 candidates = 50 inferences/sec

---

## Next Steps Ä‘á»ƒ Implement

**Phase 1: Setup SFU cÆ¡ báº£n (2-3 ngÃ y)**
1. Install aiortc dependencies
2. Create SFU service (nháº­n tracks tá»« 1 candidate)
3. Forward tracks Ä‘áº¿n proctor
4. Test WebRTC end-to-end

**Phase 2: AI Pipeline (3-5 ngÃ y)**
5. Setup YOLO face detection
6. Setup ArcFace face recognition  
7. Setup Screen OCR (Tesseract/PaddleOCR)
8. Audio VAD

**Phase 3: Integration (2-3 ngÃ y)**
9. Connect AI â†’ Rules Engine
10. WebSocket broadcast incidents
11. Recording service
12. Performance optimization

**Total**: ~7-11 ngÃ y development

---

Báº¡n muá»‘n tÃ´i báº¯t Ä‘áº§u implement tá»« bÆ°á»›c nÃ o?
